{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "This notebook will demonstrate how to use Apache Spark to recommend movies to a user. I will start with some basic techniques, and then use the Spark ML library's Alternating Least Squares method to make more sophisticated predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Data Set\n",
    "This dataset describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 20000263 ratings and 465564 tag applications across 27278 movies. These data were created by 138493 users between January 09, 1995 and March 31, 2015. This dataset was generated on October 17, 2016.\n",
    "\n",
    "Users were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.\n",
    "[Link of Data Set](http://grouplens.org/datasets/movielens/)\n",
    "\n",
    "## Reference\n",
    "F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages. DOI=http://dx.doi.org/10.1145/2827872"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Cache\n",
    "* Why do we need to call cache? [See the perfect answer](http://stackoverflow.com/questions/28981359/why-do-we-need-to-call-cache-or-persist-on-a-rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings_filename = 'ratings.csv'\n",
    "movies_filename = 'movies.csv'\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "ratings_df_schema = StructType(\n",
    "  [StructField('userId', IntegerType()),\n",
    "   StructField('movieId', IntegerType()),\n",
    "   StructField('rating', DoubleType())]\n",
    ")\n",
    "movies_df_schema = StructType(\n",
    "  [StructField('ID', IntegerType()),\n",
    "   StructField('title', StringType())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, title: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read rating data\n",
    "raw_ratings_df = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "                           .options(header=True, inferSchema=False)\\\n",
    "                           .schema(ratings_df_schema).load(ratings_filename)\n",
    "ratings_df = raw_ratings_df.drop('Timestamp')\n",
    "ratings_df.cache()\n",
    "\n",
    "# Read movie data\n",
    "raw_movies_df = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "                          .options(header=True, inferSchema=False)\\\n",
    "                          .schema(movies_df_schema).load(movies_filename)\n",
    "movies_df = raw_movies_df.drop('Genres').withColumnRenamed('movieId', 'ID')\n",
    "movies_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20000263 ratings and 27278 movies in the datasets.\n"
     ]
    }
   ],
   "source": [
    "print \"There are {} ratings and {} movies in the datasets.\".format(ratings_df.count(), movies_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings:\n",
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|      2|   3.5|\n",
      "|     1|     29|   3.5|\n",
      "|     1|     32|   3.5|\n",
      "+------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Ratings:\"\n",
    "ratings_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies:\n",
      "+---+--------------------+\n",
      "| ID|               title|\n",
      "+---+--------------------+\n",
      "|  1|    Toy Story (1995)|\n",
      "|  2|      Jumanji (1995)|\n",
      "|  3|Grumpier Old Men ...|\n",
      "+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Movies:\"\n",
    "movies_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Recommendations\n",
    "One way to recommend movies is to always recommend the movies with the highest average rating. In this part, I will use Spark to find the name, number of ratings, and the average rating of the 20 movies with the highest average rating and at least 500 reviews. We should filter our movies with high ratings but greater than or equal to 500 reviews because movies with few reviews may not have broad appeal to everyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_ids_with_avg_ratings_df:\n",
      "+-------+-----+------------------+\n",
      "|movieId|count|average           |\n",
      "+-------+-----+------------------+\n",
      "|3997   |2047 |2.0703468490473864|\n",
      "|1580   |35580|3.55831928049466  |\n",
      "|3918   |1246 |2.918940609951846 |\n",
      "+-------+-----+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "movie_ids_with_avg_ratings_df = ratings_df.groupBy('movieId')\\\n",
    "                                          .agg(F.count(ratings_df.rating).alias(\"count\"), \n",
    "                                               F.avg(ratings_df.rating).alias(\"average\"))\n",
    "print 'movie_ids_with_avg_ratings_df:'\n",
    "movie_ids_with_avg_ratings_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_names_with_avg_ratings_df:\n",
      "+-------+-----------------------------------------------+-----+-------+\n",
      "|movieId|title                                          |count|average|\n",
      "+-------+-----------------------------------------------+-----+-------+\n",
      "|125599 |Always for Pleasure (1978)                     |1    |5.0    |\n",
      "|104317 |Flight of the Conchords: A Texan Odyssey (2006)|1    |5.0    |\n",
      "|126219 |Marihuana (1936)                               |1    |5.0    |\n",
      "+-------+-----------------------------------------------+-----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movie_names_df = movie_ids_with_avg_ratings_df.join(movies_df, movie_ids_with_avg_ratings_df.movieId==movies_df.ID, 'inner')\n",
    "movie_names_with_avg_ratings_df = movie_names_df.select(['movieId', 'title', 'count', 'average'])\\\n",
    "                                                .sort('average', ascending=False)\n",
    "\n",
    "del movie_names_df\n",
    "print 'movie_names_with_avg_ratings_df:'\n",
    "movie_names_with_avg_ratings_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies with highest ratings:\n",
      "+-------+--------------------------------+-----+-----------------+\n",
      "|movieId|title                           |count|average          |\n",
      "+-------+--------------------------------+-----+-----------------+\n",
      "|318    |Shawshank Redemption, The (1994)|63366|4.446990499637029|\n",
      "|858    |Godfather, The (1972)           |41355|4.364732196832306|\n",
      "|50     |Usual Suspects, The (1995)      |47006|4.334372207803259|\n",
      "+-------+--------------------------------+-----+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_with_500_ratings_or_more = movie_names_with_avg_ratings_df.filter(\"count >= 500\")\n",
    "print 'Movies with highest ratings:'\n",
    "movies_with_500_ratings_or_more.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering\n",
    "\n",
    "<img src=\"https://courses.edx.org/c4x/BerkeleyX/CS100.1x/asset/Collaborative_filtering.gif\" alt=\"collaborative filtering\" style=\"float: right;height: 300px\"/>\n",
    "Collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue x than to have the opinion on x of a person chosen randomly\n",
    "\n",
    "The image at the right (from Wikipedia) shows an example of predicting of the user's rating using collaborative filtering. At first, people rate different items (like videos, images, games). After that, the system is making predictions about a user's rating for an item, which the user has not rated yet. These predictions are built upon the existing ratings of other users, who have similar ratings with the active user. For instance, in the image below the system has made a prediction, that the active user will not like the video.\n",
    "\n",
    "*Reference: [ALS 在 Spark MLlib 中的实现](http://mp.weixin.qq.com/s?__biz=MjM5MjAwODM4MA==&mid=206741946&idx=1&sn=cd35e124595d9d998b54a1700296419d#rd)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 11998918\n",
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|      2|   3.5|\n",
      "|     1|     29|   3.5|\n",
      "|     1|     47|   3.5|\n",
      "+------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Validation: 4001830\n",
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|     32|   3.5|\n",
      "|     1|    253|   4.0|\n",
      "|     1|    293|   4.0|\n",
      "+------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Test: 3999515\n",
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|    112|   3.5|\n",
      "|     1|    151|   4.0|\n",
      "|     1|    318|   4.0|\n",
      "+------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 1800009193L\n",
    "(split_60_df, split_a_20_df, split_b_20_df) = ratings_df.randomSplit([0.6, 0.2, 0.2], seed)\n",
    "\n",
    "# Let's cache these datasets for performance\n",
    "training_df = split_60_df.cache()\n",
    "validation_df = split_a_20_df.cache()\n",
    "test_df = split_b_20_df.cache()\n",
    "\n",
    "print 'Training: {}'.format(training_df.count())\n",
    "training_df.show(3)\n",
    "print 'Validation: {}'.format(validation_df.count())\n",
    "validation_df.show(3)\n",
    "print 'Test: {}'.format(test_df.count())\n",
    "test_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternating Least Squares\n",
    "To determine the best values for the parameters, we will use ALS to train several models, and then we will select the best model and use the parameters from that model in the rest of this notebook.\n",
    "\n",
    "#### Why are we doing our own cross-validation?\n",
    "A challenge for collaborative filtering is how to provide ratings to a new user (a user who has not provided any ratings at all). Some recommendation systems choose to provide new users with a set of default ratings (e.g., an average value across all ratings), while others choose to provide no ratings for new users. Spark's ALS algorithm yields a NaN value when asked to provide a rating for a new user.\n",
    "\n",
    "Using the ML Pipeline's CrossValidator with ALS is thus problematic, because cross validation involves dividing the training data into a set of folds (e.g., three sets) and then using those folds for testing and evaluating the parameters during the parameter grid search process. It is likely that some of the folds will contain users that are not in the other folds, and, as a result, ALS produces NaN values for those new users. When the CrossValidator uses the Evaluator (RMSE) to compute an error metric, the RMSE algorithm will return NaN. This will make all of the parameters in the parameter grid appear to be equally good (or bad).\n",
    "\n",
    "This issue has been discussed on [Spark JIRA 14489](https://issues.apache.org/jira/browse/SPARK-14489). There are proposed workarounds of having ALS provide default values or having RMSE drop NaN values. Both introduce potential issues.\n",
    "\n",
    "**For a production application, we should consider the tradeoffs in how to handle new users.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 4 the RMSE is 0.827825178948\n",
      "For rank 8 the RMSE is 0.815401279892\n",
      "For rank 12 the RMSE is 0.809404403199\n",
      "The best model was trained with rank 12\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Let's initialize our ALS learner\n",
    "als = ALS()\n",
    "\n",
    "# Set the parameters for ALS\n",
    "als.setMaxIter(5)\\\n",
    "   .setSeed(seed)\\\n",
    "   .setRegParam(0.1)\\\n",
    "   .setUserCol('userId')\\\n",
    "   .setItemCol('movieId')\\\n",
    "   .setRatingCol('rating')\n",
    "\n",
    "# Create an RMSE evaluator using the label and predicted columns\n",
    "reg_eval = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n",
    "\n",
    "ranks = [4, 8, 12]\n",
    "min_error = float('inf')\n",
    "best_model = None\n",
    "for rank in ranks:\n",
    "    als.setRank(rank)\n",
    "    # Create the model with these parameters.\n",
    "    model = als.fit(training_df)\n",
    "    # Predict against the validation_df.\n",
    "    predict_df = model.transform(validation_df)\n",
    "    # Remove NaN values from prediction\n",
    "    predicted_ratings_df = predict_df.filter(predict_df.prediction != float('nan'))\n",
    "\n",
    "    error = reg_eval.evaluate(predicted_ratings_df)\n",
    "    print 'For rank %s the RMSE is %s' % (rank, error)\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_model = model\n",
    "\n",
    "print 'The best model was trained with rank {}'.format(best_model.rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our model to see if this model would be overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model had a RMSE on the test set of 0.808681089623\n"
     ]
    }
   ],
   "source": [
    "predict_df = best_model.transform(test_df)\n",
    "\n",
    "# Remove NaN values from prediction\n",
    "predicted_test_df = predict_df.filter(predict_df.prediction != float('nan'))\n",
    "del predict_df\n",
    "\n",
    "# Evaluation\n",
    "test_RMSE = reg_eval.evaluate(predicted_test_df)\n",
    "\n",
    "print 'The model had a RMSE on the test set of {}'.format(test_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for Myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My movie ratings:\n",
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|0     |858    |4.0   |\n",
      "|0     |7502   |4.8   |\n",
      "|0     |58559  |5.0   |\n",
      "+------+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_user_id = 0\n",
    "\n",
    "my_rated_movies = [\n",
    "     (my_user_id, 858, 4.0),\n",
    "     (my_user_id, 7502, 4.8),\n",
    "     (my_user_id, 58559, 5.0)\n",
    "]\n",
    "\n",
    "my_ratings_df = sqlContext.createDataFrame(my_rated_movies, ['userId','movieId','rating'])\n",
    "print 'My movie ratings:'\n",
    "my_ratings_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding my rating data into training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset now has 3 more entries than before.\n"
     ]
    }
   ],
   "source": [
    "training_with_my_ratings_df = training_df.unionAll(my_ratings_df)\n",
    "print 'The training dataset now has {} more entries than before.'.format(training_with_my_ratings_df.count() - training_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rank = 12\n",
    "my_ratings_model = als.setRank(rank).fit(training_with_my_ratings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating my model on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model had a RMSE on the test set of 0.809872586652\n"
     ]
    }
   ],
   "source": [
    "my_predict_df = my_ratings_model.transform(test_df)\n",
    "\n",
    "# Remove NaN values from prediction\n",
    "predicted_test_my_ratings_df = my_predict_df.filter(my_predict_df.prediction != float('nan'))\n",
    "del my_predict_df\n",
    "\n",
    "test_RMSE_my_ratings = reg_eval.evaluate(predicted_test_my_ratings_df)\n",
    "print 'The model had a RMSE on the test set of {}'.format(test_RMSE_my_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the top 5 recommened movies from my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter out the movies I already rated.\n",
    "not_rated_df = movies_df.filter(~movies_df.ID.isin([x[1] for x in my_rated_movies]))\n",
    "\n",
    "# Rename the \"ID\" column to be \"movieId\"\n",
    "my_unrated_movies_df = not_rated_df.withColumnRenamed('ID', 'movieId')\n",
    "\n",
    "# Add a new column, userId, with my_user_id\n",
    "my_unrated_movies_df = my_unrated_movies_df.withColumn('userId', F.lit(my_user_id))\n",
    "\n",
    "# Use my_rating_model to predict ratings for the movies that I did not manually rate.\n",
    "raw_predicted_ratings_df = my_ratings_model.transform(my_unrated_movies_df)\n",
    "\n",
    "predicted_ratings_df = raw_predicted_ratings_df.filter(raw_predicted_ratings_df['prediction'] != float('nan'))\n",
    "del raw_predicted_ratings_df\n",
    "\n",
    "predicted_with_counts_df = predicted_ratings_df.join(movie_names_with_avg_ratings_df, [\"movieId\", \"title\"], \"inner\")\n",
    "predicted_highest_rated_movies_df = predicted_with_counts_df.filter('count > 75')\\\n",
    "                                                            .select(['movieId', 'title', 'prediction', \n",
    "                                                                     'average', 'count'])\\\n",
    "                                                            .sort('prediction', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My top 5 rated movies as predicted:\n",
      "+-------+---------------------------------------------------------+----------+-----------------+-----+\n",
      "|movieId|title                                                    |prediction|average          |count|\n",
      "+-------+---------------------------------------------------------+----------+-----------------+-----+\n",
      "|7153   |Lord of the Rings: The Return of the King, The (2003)    |4.6338167 |4.14238211356367 |31577|\n",
      "|79132  |Inception (2010)                                         |4.5958385 |4.156172003137702|14023|\n",
      "|318    |Shawshank Redemption, The (1994)                         |4.593068  |4.446990499637029|63366|\n",
      "|4993   |Lord of the Rings: The Fellowship of the Ring, The (2001)|4.576647  |4.137925065906852|37553|\n",
      "|5952   |Lord of the Rings: The Two Towers, The (2002)            |4.5753546 |4.107520546734616|33947|\n",
      "+-------+---------------------------------------------------------+----------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'My top 5 rated movies as predicted:'\n",
    "predicted_highest_rated_movies_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
